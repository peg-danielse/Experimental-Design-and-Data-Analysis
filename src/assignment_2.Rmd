---
title: "assignment 2"
author: "PD : 2737660, JO: 2672027, MD: 2641423, GR: 70"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6, fig.height = 3)
```

# Assignment 2

## Exercise 1

```{r}
tree = read.table("../datasets/treeVolume.txt", header=TRUE)
```

**a)**

You can only perform a t-test if the data contains two groups. Which is
the case for the tree volumes since there are two types of trees.

```{r}
treeframe <- data.frame(volume=(tree$volume), 
                        type=factor(tree$type))
treeanov=lm(volume~type ,data = treeframe)
anova(treeanov)
summary(treeanov)
par(mfrow=c(1,2)); qqnorm(residuals(treeanov)); qqline(residuals(treeanov))
plot(fitted(treeanov),residuals(treeanov))
```

The estimated mean for the tree types is 30.171 for beeches and 35.250
for oaks. Which is the same as the estimated mean from the anova
summary.

```{r}
t.test(volume~type, data = tree)
```

**b)** To investigate the influence of diameter on the different tree
types we include both diameter and type in the linear model. By doing an
ANOVA test we can conclude that there in no difference between the
influence of diameter between the different types. This model seems to
explain the data well with an r-squared value of 0.92.

To investigate the influence of height on the different tree types we
include both height and type in the linear model. By doing an ANOVA test
we can conclude that there in no difference between the influence of
height between the different types. The model also appears to be a bad
one with an r-squared value of only 0.22.

```{r}
treeframe <- data.frame(volume=(tree$volume), 
                        type=factor(tree$type),
                        diameter=(tree$diameter),
                        height=(tree$height))

treeanovheight=lm(volume~height+type, data = treeframe)
treeanovdiameter=lm(volume~diameter+type, data = treeframe)

anova(treeanovheight)
anova(treeanovdiameter)

summary(treeanovheight)
summary(treeanovdiameter)
```

**c)**

since both the tests from **b)** conclude that there is no difference
between the influence of diameter and height on volume for the different
types we can simply remove type from our model. This results in a better
fit. We will then use our model to predict a value using the (overall)
mean diameter and mean height.

```{r}

treeanovdh=lm(volume~diameter+height, data = treeframe)

summary(treeanovdh);

pairs(treeframe)

newxdata = data.frame(diameter=mean(treeframe$diameter), height=mean(treeframe$height))
predict(treeanovdh, newxdata)
  
```

**d)** we propose that the interaction between diameter and height will
be included in the model. this way we get a better fit. explanatory
values are significant.

```{r}
treeanov=lm(volume~diameter*height, data = treeframe)
anova(treeanov)
summary(treeanov)

```

## Problem 2 

```{r}
crimedf = read.table("../datasets/expensescrime.txt", header=TRUE)
```

```{r}
par(mfrow=c(3,1))
pairs(~pop+lawyers+expend+employ+bad+crime+pop, data=crimedf)
# pairs(~bad+crime+pop, data=crime)
```

**a)** judging from the plot we can clearly see some colinearity between
all values exept crime. We can also spot some possible influence points.
We investigate the influence points by looking at the Cooks distance. We
found 4 influence points, with index 5 (CA), 8 (DC), 35 (NY), 44 (TX).
These we remove from the data. We checked co-linearity with the vif
function which returned concerning values for employ, pop and to a
lesser extend lawyers and bad. Which confirms what we expected from
looking at the pairs plot. We also checked the colinearity of crime and
the other data points and the vif indicates that it does not have a
colinearity problem.

```{r}
totallm = lm(expend~bad+crime+lawyers+employ+pop, data=crimedf)
cooks = round(cooks.distance(totallm),2)
indexes = which(cooks > 1)
plot(1:51,cooks.distance(totallm),type="b")

crime_w = crimedf[,-1]
crime = crime_w[-indexes, ]

#colinearity
library(car)
vif(totallm)
crimelm = lm(expend~crime+employ, data=crime)
vif(crimelm)
summary(crimelm)
```

```{r}
crimelm_le = lm(expend~lawyers+employ, data=crime)
vif(crimelm_le)
crimelm_cp = lm(expend~crime+pop, data=crime)
vif(crimelm_cp)
```

**b)** using the step-up method we can keep adding variables to the base
model. since we show in **a** that there is colinearity between most of
the variables we can not add them even if they result in a higher $R^2$
value. We end up with a model that just uses employ.

you can improve the prediction interval by reducing the variance of the
data. For example this could be done by removing the influence points
that effect the linear model which we found in a). with this we get a
smaller prediction interval.

**c)** Determine a 95% prediction interval for the expend using the
model you preferred in b) for a (hypothetical) state with bad=50,
crime=5000, lawyers=5000, employ=5000 and pop=5000. Can you improve this
interval?

you can improve the prediction interval by reducing the variance of the
data. This we cannot do since we removed the influence points already.

```{r}
xnew = data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
predict(crimelm, xnew, interval="prediction")

```

**d)**

We applied lasso to the data set. Here we found that lasso uses the
values that we classified to be co-linear. Therefore this does produce a
similar model but we can show by comparing the lasso model with our
model that the lasso model is significantly different from ours and that
therefore we cannot prefer our model because it contains less variables.
There might be some explanatory value we missed by not including the
variables we thought to be co linear.

```{r}
library(glmnet)
library(Matrix)

y = crime$expend
x = crime[,-c(1,2)]

train=sample(1:nrow(x),0.67*nrow(x)) # train by using 2/3 of the data
x.train=x[train,]; y.train=y[train] # data to train
x.test=x[-train,]; y.test=y[-train] # data to test the prediction quality
lasso.mod=glmnet(x.train,y.train,alpha=1)
cv.lasso=cv.glmnet(as.matrix(x.train),y.train,alpha=1,type.measure="mse")
plot(lasso.mod,label=T,xvar="lambda") #have a look at the lasso path
plot(cv.lasso) # the best lambda by cross-validation
plot(cv.lasso$glmnet.fit,xvar="lambda",label=T)

lambda.min=cv.lasso$lambda.min; lambda.1se=cv.lasso$lambda.1se
coef(lasso.mod,s=cv.lasso$lambda.min) #betaâ€™s for the best lambda
y.pred=predict(lasso.mod,s=lambda.min,newx=as.matrix(x.test)) #predict for test
mse.lasso=mean((y.test-y.pred)^2) #mse for the predicted test rows

anova(lm(expend~crime+lawyers+employ+pop, data=crime), crimelm)

```

## Exercise 3

```{r}
titanic = read.table("../datasets/titanic.txt", header=TRUE)
```

**a)**

**Oh wijze paul check dit ik moet,ik denk dat estm. in de uilteg verder
gekwantificieerd moet worden**

Looking at the the data we can drawn several conclusions.

-   Starting with PClass, passengers in the first class had the
    strongest change of survival. The survival rate drops when the
    PClass is the 2nd class exp(-1.29) indicates that there is 72.5%
    less change of survival then a passenger from the first class. The
    chance of survival drops more strongly when it is a passenger from
    the 3rd class. A 3rd class has a survival rate of exp(-2.52) which
    it 92% less likely to survive compared to the first class.

-   The chance of survival drops every year a passenger is older. For
    every the chance of survival drop by exp(-0.0391) which means 0.04%
    a year.

-   Being male strongly decreases the chance of the passenger surviving.
    A male passenger has a exp(-2.63) which means there was a 92% less
    chance of survival compared to female.s

```{r}
summary(titanic)

titanic$PClass <- factor(titanic$PClass)
titanic$Sex <- factor(titanic$Sex)
titanic$Survived <- factor(titanic$Survived)

tot = xtabs(~PClass + Sex, data=titanic)
# tot.c = xtabs(Survived~ PClass + Sex, data=titanic)
# round(tot.c/tot,2)


model <- glm(Survived ~ PClass + Age + Sex, data=titanic, family=binomial())
summary(model)



```

**b)**

**Hier moet nog uitleg bij**

```{r}
model2 <- glm(Survived ~ PClass*Age* Sex, data=titanic, family=binomial())
summary(model2)
```

| Index | PClass | Sex    |
|-------|--------|--------|
| 1     | 1st    | female |
| 2     | 2nd    | female |
| 3     | 3rd    | female |
| 4     | 1st    | male   |
| 5     | 2nd    | male   |
| 6     | 3rd    | male   |

The results seem to be inline with results from the previous exercise.
Being female and travelling in a higher class seem to positively impact
the chance of survival.

```{r}
age <- 55
newdata <- expand.grid(PClass = c("1st", "2nd", "3rd"), Sex = c("female", "male"))
newdata$Age <- age
predict(model, newdata, type = "response")
```

**C)**

**Is op basis van chatgpt dus diend gecheckt te worden**

To predict the survival status of passengers on the Titanic a binary
classification model can be used such as logistic regression. This model
can take in the passenger information such as age, sex, and passenger
class as input features and predict whether the passenger survived or
not as the output label.

To evaluate the quality of the prediction, we can use metrics such as
accuracy, precision, recall, and F1 score.

**D)**

-   Null hypothesis (H0): There is no relation between surviving the
    titanic and class

-   Alternative hypothesis (Ha): There is a relation between surviving
    the titanic and class

The p-value of the test is less than 0.05 which that we can reject H0.
This indicates that there is significant association between a passenger
surviving and there class.

-   Null hypothesis (H0): There is no relation between surviving the
    titanic and gender

-   Alternative hypothesis (Ha): There is a relation between surviving
    the titanic and gender

The p-value of the test is less than 0.05 which that we can reject H0.
This indicates that there is significant association between a passenger
surviving and there gender.

```{r}
cont_table_class <- table(titanic$Survived, titanic$PClass)
cont_table_class

chisq.test(as.matrix(cont_table_class))

cont_table_sex <- table(titanic$Survived, titanic$Sex)
cont_table_sex

chisq.test(as.matrix(cont_table_sex))
```

**E)**

**Is op basis van chatgpt dus diend gecheckt te worden**

The second approach is not wrong but they different advantages and
disadvantages. A property that contingency table test has is that gives
insight on strength and direction of an association. A limitation on the
other hand is that it does not account for more complex relationships
between factors, also it does not provide a predictive model.

## Exercise 4

```{r}
coups = read.table("../datasets/coups.txt", header=TRUE)
```

**a)**

beetje onduidelijk of het nou increasing of decreasing factoren zijn
maar vgm increasing, ik twijfel of pollib als factor moet of niet, had
het eerst wel maar denk toch van niet

Todo: plot, text mss nog wat uitbreiden

Looking at the Poisson regression model summary , we can determine how
much each unit impacts the variables related to the response variable.
Based on significance there are only three variables that seem to have
an impact. The years a country has been oligarchy and the number of
parties a country has seem to positive association with number of coups.
A country having a full civil rights has negative association with
coups.

```{r}
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family=poisson, data=coups)
summary(coupsglm)
#Add pairs diagram
pairs(coups)

```

**b)**

After performing the step down approach we are left with same three
significant variables. As in the model in exercise a.

```{r}
#step 1
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim, family=poisson, data=coups)
#step 2 
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size, family=poisson, data=coups)
#step 3
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family=poisson, data=coups)
#step 4
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote, family=poisson, data=coups)
# #step 5
coupsglm = glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=coups)
summary(coupsglm)
```

**c)**

Ik weet niet of alles in het model moet meenemen of dat je variabelen
die niet het model zitten moet weg laten

Based on model B the fictional country with lowest level of of civil
right has the highest chance for a coup occurring based on the
prediction

```{r}
newdata <- data.frame(oligarchy = mean(coups$oligarchy), pollib = c(0, 1, 2), parties = mean(coups$parties))

pred = predict(coupsglm, newdata, type = "response", se.fit = TRUE)

barplot(pred$fit)
```
