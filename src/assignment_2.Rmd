---
title: "assignment 2"
author: "PD : 2737660, JO: 2672027, MD: 2641423, GR: 70"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6, fig.height = 3)
```

## Problem 1

```{r}
tree = read.table("../datasets/treeVolume.txt", header=TRUE)
```

**a)**

You can only perform a t-test if the data contains two groups. Which is
the case for the tree volumes since there are two types of trees.

```{r}
treeframe <- data.frame(volume=(tree$volume), 
                        type=factor(tree$type))
treeanov=lm(volume~type ,data = treeframe)
anova(treeanov)
summary(treeanov)
par(mfrow=c(1,2)); qqnorm(residuals(treeanov)); qqline(residuals(treeanov))
plot(fitted(treeanov),residuals(treeanov))
```

The estimated mean for the tree types is 30.171 for beeches and 35.250
for oaks. Which is the same as the estimated mean from the anova
summary.

```{r}
t.test(volume~type, data = tree)
```

**b)**
To investigate the influence of diameter on the different tree types we include both diameter and type in the linear model. By doing an ANOVA test we can conclude that there in no difference between the influence of diameter between the different types. This model seems to explain the data well with an r-squared value of 0.92.

To investigate the influence of height on the different tree types we include both height and type in the linear model. By doing an ANOVA test we can conclude that there in no difference between the influence of height between the different types. The model also appears to be a bad one with an r-squared value of only 0.22.

```{r}
treeframe <- data.frame(volume=(tree$volume), 
                        type=factor(tree$type),
                        diameter=(tree$diameter),
                        height=(tree$height))

treeanovheight=lm(volume~height+type, data = treeframe)
treeanovdiameter=lm(volume~diameter+type, data = treeframe)

anova(treeanovheight)
anova(treeanovdiameter)

summary(treeanovheight)
summary(treeanovdiameter)
```

**c)**

since both the tests from **b)** conclude that there is no difference between the influence of diameter and height on volume for the different types we can simply remove type from our model. This results in a better fit. We will then use our model to predict a value using the (overall) mean diameter and mean height.  

```{r}

treeanovdh=lm(volume~diameter+height, data = treeframe)

summary(treeanovdh);

pairs(treeframe)

newxdata = data.frame(diameter=mean(treeframe$diameter), height=mean(treeframe$height))
predict(treeanovdh, newxdata)
  
```

**d)**
we propose that the interaction between diameter and height will be included in the model. this way we get a better fit. explanatory values are significant.
```{r}
treeanov=lm(volume~diameter*height, data = treeframe)
anova(treeanov)
summary(treeanov)

```

## Problem 2

```{r}
y = crime2$expend
x = crime2[,-c(1,2)]
```


```{r}
crimedf = read.table("../datasets/expensescrime.txt", header=TRUE)
```
```{r}
par(mfrow=c(3,1))
pairs(~pop+lawyers+expend+employ+bad+crime+pop, data=crime)
# pairs(~bad+crime+pop, data=crime)
```

**a)**
judging from the plot we can clearly see some colinearity between all values 
exept crime. We can also spot some possible influence points.
We investigate the influence points by looking at the Cooks distance. We
found 4 influence points, with index 5 (CA), 8 (DC), 35 (NY), 44 (TX). These
we remove from the data.
We checked co-linearity with the vif function which returned concerning
values for employ, pop and to a lesser extend lawyers and bad. Which confirms
what we expected from looking at the pairs plot. We also checked the colinearity
of crime and the other data points and the vif indicates that it does not have 
a colinearity problem.

```{r}
totallm = lm(expend~bad+crime+lawyers+employ+pop, data=crime)
cooks = round(cooks.distance(totallm),2)
indexes = which(cooks > 1)
plot(1:51,cooks.distance(crimelm),type="b")

crime = crimedf[-indexes, ]

#colinearity
library(car)
vif(crimelm)
crimelm3 = lm(expend~crime+employ, data=crime)
vif(crimelm3)
summary(crimelm3)
```

```{r}
crimelm3_le = lm(expend~lawyers+employ, data=crime)
vif(crimelm3_le)
crimelm3_cp = lm(expend~crime+pop, data=crime)
vif(crimelm3_cp)
```

**b)**
using the step-up method we can keep adding variables to the base model. since
we show in **a** that there is colinearity between most of the variables we can
not add them even if they result in a higher $R^2$ value. We end up with a model
that just uses employ.


```{r} 

crime = crimedf[,-1]

crimelm = lm(expend~employ ,data = crime)

summary(crimelm)

```

**c)**
Determine a 95% prediction interval for the expend using the model you preferred in b) for a (hypothetical) state with bad=50, crime=5000, lawyers=5000, employ=5000 and pop=5000. Can you improve this interval?

you can improve the prediction interval by reducing the variance of the data. This we cannot do since we removed the influence points already.

```{r} 
xnew = data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)

predict(crimelm, xnew, interval="prediction")

crime2lm = lm(expend~employ ,data = crime2)
predict(crime2lm, xnew, interval="prediction")

```

**d)**
Apply the LASSO method to choose the relevant variables (with default parameters as in the lecture and lambda=lambda.1se). (You will need to install the R-package glmnet, which is not included in the standard distribution of R.) Compare the resulting model with the model obtained in b). (Beware that in general a new run delivers a new model because of a new train set.) 

```{r} 
library(glmnet)

y = crime2$expend
x = crime2[,-c(1,2)]


```
