---
title: "assignment 2"
author: "Paul Danielse : 2737660, Joshua Offermans: 2672027, Maurits Dijk: 2641423, GR: 70"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6, fig.height = 3)
```

# Assignment 2

## Exercise 1

```{r}
tree = read.table("../datasets/treeVolume.txt", header=TRUE)
```

**a)** To investigate the differences in tree types we want to use a
t-test. You can only perform a t-test if the data contains two groups.
Which is the case for the tree volumes since there are two types of
trees. Therefore, it is similar to the T-test in the results but not in
the statistic used for arriving at the result since anova uses an f
distributed statistic.

```{r}
treeframe <- data.frame(volume=(tree$volume), 
                        type=factor(tree$type))
treeanov=lm(volume~type ,data = treeframe)
anova(treeanov)
summary(treeanov)
par(mfrow=c(1,2)); qqnorm(residuals(treeanov)); qqline(residuals(treeanov))
plot(fitted(treeanov),residuals(treeanov))
```

The estimated mean for the tree types is 30.171 for beeches and 35.250
for oaks. Which is the same as the estimated mean from the anova
summary.

```{r}
t.test(volume~type, data = tree)
```

**b)** To investigate the influence of diameter on the different tree
types we include both diameter and type in the linear model. By doing an
ANOVA test we can conclude that there in no difference between the
influence of diameter between the different types. This model seems to
explain the data well with an r-squared value of 0.92.

To investigate the influence of height on the different tree types we
include both height and type in the linear model. By doing an ANOVA test
we can conclude that there in no difference between the influence of
height between the different types. The model also appears to be a bad
one with an r-squared value of only 0.22.

```{r}
treeframe <- data.frame(volume=(tree$volume), 
                        type=factor(tree$type),
                        diameter=(tree$diameter),
                        height=(tree$height))

treeanovheight=lm(volume~height+type, data = treeframe)
treeanovdiameter=lm(volume~diameter+type, data = treeframe)

anova(treeanovheight)
anova(treeanovdiameter)

summary(treeanovheight)
summary(treeanovdiameter)
```

**c)** Since both the tests from **b)** conclude that there is no
difference between the influence of diameter and height on volume for
the different types we can simply remove type from our model. This
results in a better fit. We will then use our model to predict a value
using the (overall) mean diameter and mean height.

```{r}

treeanovdh=lm(volume~diameter+height, data = treeframe)

summary(treeanovdh);

pairs(treeframe)

newxdata = data.frame(diameter=mean(treeframe$diameter), height=mean(treeframe$height))
predict(treeanovdh, newxdata)
  
```

**d)** We propose that the interaction between diameter and height will
be included in the model. this way we get a better fit. explanatory
values are significant.

```{r}
treeanov=lm(volume~diameter*height, data = treeframe)
anova(treeanov)
summary(treeanov)

```

## Exercise 2

```{r}
crimedf = read.table("../datasets/expensescrime.txt", header=TRUE)
```

```{r}
par(mfrow=c(3,1))
pairs(~pop+lawyers+expend+employ+bad+crime+pop, data=crimedf)
# pairs(~bad+crime+pop, data=crime)
```

**a)** Judging from the plot we can clearly see some co-linearity
between all values except crime. We can also spot some possible
influence points. We investigate the influence points by looking at the
Cooks distance. We found 4 influence points, with index 5 (CA), 8 (DC),
35 (NY), 44 (TX). These we remove from the data. We checked co-linearity
with the vif function which returned concerning values for employ, pop
and to a lesser extend lawyers and bad. Which confirms what we expected
from looking at the pairs plot. We also checked the co-linearity of
crime and the other data points and the vif indicates that it does not
have a co-linearity problem.

```{r}
totallm = lm(expend~bad+crime+lawyers+employ+pop, data=crimedf)
cooks = round(cooks.distance(totallm),2)
indexes = which(cooks > 1)
plot(1:51,cooks.distance(totallm),type="b")

crime_w = crimedf[,-1]
crime = crime_w[-indexes, ]

#colinearity
library(car)
vif(totallm)
crimelm = lm(expend~crime+employ, data=crime)
vif(crimelm)
summary(crimelm)
```

```{r}
crimelm_le = lm(expend~lawyers+employ, data=crime)
vif(crimelm_le)
crimelm_cp = lm(expend~crime+pop, data=crime)
vif(crimelm_cp)
```

**b)** Using the step-up method we can keep adding variables to the base
model. since we show in **a)** that there is co-linearity between most
of the variables we can not add them even if they result in a higher
$R^2$ value. We end up with a model that just uses employ.

you can improve the prediction interval by reducing the variance of the
data. For example this could be done by removing the influence points
that effect the linear model which we found in **a)**. with this we get
a smaller prediction interval.

**c)** We cannot improve this confident interval since we removed the
influence points already.

```{r}
xnew = data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
predict(crimelm, xnew, interval="prediction")

```

**d)** We applied lasso to the data set. Here we found that lasso uses
the values that we classified to be co-linear. Therefore this does
produce a similar model but we can show by comparing the lasso model
with our model that the lasso model is significantly different from ours
and that therefore we cannot prefer our model because it contains less
variables. There might be some explanatory value we missed by not
including the variables we thought to be co linear.

```{r}
library(glmnet)
library(Matrix)

y = crime$expend
x = crime[,-c(1,2)]

train=sample(1:nrow(x),0.67*nrow(x)) # train by using 2/3 of the data
x.train=x[train,]; y.train=y[train] # data to train
x.test=x[-train,]; y.test=y[-train] # data to test the prediction quality
lasso.mod=glmnet(x.train,y.train,alpha=1)
cv.lasso=cv.glmnet(as.matrix(x.train),y.train,alpha=1,type.measure="mse")
plot(cv.lasso) # the best lambda by cross-validation
plot(cv.lasso$glmnet.fit,xvar="lambda",label=T)

lambda.min=cv.lasso$lambda.min; lambda.1se=cv.lasso$lambda.1se
coef(lasso.mod,s=cv.lasso$lambda.min) #betaâ€™s for the best lambda
y.pred=predict(lasso.mod,s=lambda.min,newx=as.matrix(x.test)) #predict for test
mse.lasso=mean((y.test-y.pred)^2) #mse for the predicted test rows

anova(lm(expend~crime+lawyers+employ+pop, data=crime), crimelm)

```

## Exercise 3

```{r}
titanic = read.table("../datasets/titanic.txt", header=TRUE)
titanic$PClass <- factor(titanic$PClass)
titanic$Sex <- factor(titanic$Sex)

```

**a)** We investigated the probability of surviving based on the
passenger class. This is shown in the table below. The probability of
surviving is clearly related to class as people in the 1st class have a
60% chance of survival where as the survival rate for 3rd class is only
19%.

```{r}

tot = xtabs(~PClass, data=titanic)
tot.c = xtabs(Survived~PClass, data=titanic)
round(tot.c/tot,2)

```

In the bar chart below the survival chance based on the age of the
passengers is plotted. From this we can see that the probability of
surviving as an infant is higher than the middle age ranges. In the
histogram for the age data we can also see that there are less infants
on board the titanic.

```{r}


tot = xtabs(~ Age, data=titanic)
tot.c =xtabs(Survived ~ Age, data=titanic)
par(mfrow=c(1,2))
hist(titanic$Age)
barplot(round(tot.c/tot,2))

```

Lastly, we looked at the chance for survival for the different sexes. As
shown in the plot below the chance for survival are higher for women
than for men.

```{r}
tot = xtabs(~ Sex, data=titanic)
tot.c =xtabs(Survived ~ Sex, data=titanic)
barplot(round(tot.c/tot,2))

```

The model we chose includes all variables since they are all
significant.

```{r}
model <- glm(Survived ~ PClass + Age + Sex, data=titanic, family=binomial())
summary(model)

```

**b)** We investigated the interaction between Class and Sex and between
Age and sex. then we used drop1 to test if these interactions are
significant for our model. It is not in the case of Class and Age so we
cannot add that to our model. However the interaction between Age and
Sex is significant. With this we choose a model with all variables and
the interaction between age and sex.

```{r}
interaction <- glm(Survived ~ PClass + Age + Sex + PClass:Age +  Age:Sex,
                   data=titanic, family=binomial())
drop1(interaction, test = "Chisq")

final_model = glm(Survived ~ PClass + Age + Sex + Age:Sex, data=titanic, family=binomial())

```

| Index | PClass | Sex    |
|-------|--------|--------|
| 1     | 1st    | female |
| 2     | 2nd    | female |
| 3     | 3rd    | female |
| 4     | 1st    | male   |
| 5     | 2nd    | male   |
| 6     | 3rd    | male   |

The results from the prediction seem to be inline with results from
**a)**. Being female and travelling in a higher class seem to positively
impact the chance of survival. Conversely, being male and traveling in
the lowest class seems to give you an insignificant chance of survival.

```{r}
age <- 55
newdata <- expand.grid(PClass = c("1st", "2nd", "3rd"), Sex = c("female", "male"))
newdata$Age <- age
predict(final_model, newdata, type = "response")
```

**c)** To predict the survival status of hypothetical passengers we can
simply use the model chosen in **b)** to calculate the probability of
survival. With this probability we can apply a thresh hold of 50%. If
the hypothetical passenger has a probability of survival that is higher
than the thresh hold we can predict that they will survive.

The quality of the prediction is related to the distance the thresh hold
has to the probability given by the model. If the probability of
survival is slightly higher than 50% we predict all the hypothetical
passengers to survive. This is not necessarily accurate and the close we
get to this thresh hold the more inaccurate our prediction becomes since
we arrive at a binary choice of either surviving or not.

**d)**

-   Null hypothesis (H0): There is no relation between surviving the
    titanic and class

-   Alternative hypothesis (Ha): There is a relation between surviving
    the titanic and class

The p-value of the test is less than 0.05 which that we can reject H0.
This indicates that there is significant association between a passenger
surviving and their class.

```{r}
cont_table_class <- table(titanic$Survived, titanic$PClass)
cont_table_class

chisq.test(as.matrix(cont_table_class))
```

-   Null hypothesis (H0): There is no relation between surviving the
    titanic and gender

-   Alternative hypothesis (Ha): There is a relation between surviving
    the titanic and gender

The p-value of the test is less than 0.05 which that we can reject H0.
This indicates that there is significant association between a passenger
surviving and their gender.

```{r}
cont_table_sex <- table(titanic$Survived, titanic$Sex)
cont_table_sex

chisq.test(as.matrix(cont_table_sex))
```

**e)** The second approach is not wrong but there are different
advantages and disadvantages. A property that contingency table tests
have is that they are able to easily give insight on the significance
and direction of the different levels . A limitation on the other hand
is that it does not account for more complex relationships between
factors, also it does not provide a predictive model which the logistic
regression does.

## Exercise 4

```{r}
coups = read.table("../datasets/coups.txt", header=TRUE)
```

**a)** Looking at the Poisson regression model summary , we can
determine how much each unit impacts the variables related to the
response variable. Based on significance there are only three variables
that seem to have an impact. The years a country has been an oligarchy
and the number of parties a country has, appears to positively correlate
with the number of coups. Having limited civil rights does not have a
significant effect on the numbers of coups, having full civil rights
however has a negative association with coups.

```{r}
pairs(~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, data=coups)
coups$pollib <- as.factor(coups$pollib)
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family=poisson, data=coups)
summary(coupsglm)
#Add pairs diagram
pairs(coups)

```

**b)** After performing the step down approach we are left with same
three significant variables. As in the model in exercise a.

```{r}
#step 1
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim, family=poisson, data=coups)
#step 2 
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size, family=poisson, data=coups)
#step 3
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family=poisson, data=coups)
#step 4
coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote, family=poisson, data=coups)
# #step 5
coupsglm = glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=coups)
summary(coupsglm)
```

**c)** Based on model B the fictional country with lowest level of of
civil rights (1) has the highest chance for a coup occurring based on
the prediction.

```{r}
newdata <- data.frame(oligarchy = mean(coups$oligarchy), 
                      pollib = as.factor(c(0, 1, 2)), 
                      parties = mean(coups$parties))
pred = predict(coupsglm, newdata, type = "response")
pred
```
